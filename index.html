

<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Avatar-based Multimodal Empathetic Conversation.">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>The ACM Multimedia Asia 2025 Grand Challenge: Multimodal Multiethnic Talking-Head Video Generation</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@1.0.0/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <!--  <script src="./static/js/index.js"></script>-->
    <style>
        /* --- 1. å…¨å±€æ ·å¼å’Œé¢œè‰²ä¿®æ­£ --- */
        
        /* è®¾ç½®é¡µé¢åŸºç¡€æ–‡æœ¬ï¼ˆæ­£æ–‡ã€åˆ—è¡¨ç­‰ï¼‰ä¸ºçº¯é»‘è‰² */
        body, p, .content, li, td, .team-card, strong {
            color: #000000 !important;
        }
    
        /* è®¾ç½®æ‰€æœ‰æ ‡é¢˜å’Œè¡¨æ ¼å¤´ä¸ºçº¯é»‘è‰² */
        h1, h2, h3, h4, h5, h6, .title, .subtitle, th {
            color: #000000 !important;
        }
    
        /* --- 2. ã€æ ¸å¿ƒã€‘æŒ‰é’®æ ·å¼ä¿®æ­£ --- */
        /* å°†å¯¼èˆªæŒ‰é’®è®¾ç½®ä¸ºæ·±ç°è‰²èƒŒæ™¯å’Œç™½è‰²å­—ä½“ */
        .publication-links .button.is-dark {
            background-color: #4a4a4a !important; /* æ·±ç°è‰²èƒŒæ™¯ */
            color: #ffffff !important;           /* ã€å…³é”®ã€‘ç™½è‰²å­—ä½“ */
            border-color: transparent !important;
        }
    
        /* æŒ‰é’®é¼ æ ‡æ‚¬åœæ•ˆæœ */
        .publication-links .button.is-dark:hover {
            background-color: #6a6a6a !important;
        }
    
    
        /* --- 3. åŸå§‹å¸ƒå±€å’Œå…¶ä»–ç‰¹æ®Šé¢œè‰²æ ·å¼ --- */
        
        /* JSONè¯­æ³•é«˜äº® (ä¿ç•™ç‰¹æ®Šé¢œè‰²) */
        pre {outline: 1px solid #ccc; }
         .string { color: green !important; }
         .number { color: darkorange !important; }
         .boolean { color: blue !important; }
         .null { color: magenta !important; }
         .key { color: red !important; }
    
        /* è¡¨æ ¼å¸ƒå±€ */
        ._table{width: 100%; border-collapse: collapse; border:0px;}
        ._table thead tr {font-size: 13px; text-align: center; background-color: rgba(230, 255, 250, 0.92); font-weight:bold;}
        ._table td{line-height: 20px; text-align: center; padding: 4px 10px 3px 10px; height: 18px;border: 0px solid #ffffff;}
        ._table tbody tr {background: #fff; font-size: 13px;}
        ._table tbody tr:nth-child(2n){ background: #f3f3f3;}
    
        /* é¡µé¢åŸºç¡€å¸ƒå±€ */
        body {
          font-family: "Arial", sans-serif;
          background-color: #f9f9f9;
          margin: 0;
          padding: 2em;
        }
    
        /* æ ‡é¢˜å¸ƒå±€ */
        h1 {
            text-align: center;
            font-size: 2.5em;       
            font-weight: 800;       
            margin-bottom: 1em;
        }
    
        /* å›¢é˜Ÿå¡ç‰‡å¸ƒå±€ */
        .team-container {
          display: grid;
          grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
          gap: 1em;
          max-width: 900px;
          margin: 2em auto;
        }
    
        .team-card {
          background-color: #ffffff;
          border: 1px solid #ddd;
          border-radius: 10px;
          padding: 1em;
          text-align: center;
          font-weight: bold;
          box-shadow: 0 2px 5px rgba(0, 0, 0, 0.05);
          transition: transform 0.2s ease;
        }
    
        .team-card:hover {
          transform: translateY(-3px);
          box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
        }
    </style>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">The ACM Multimedia Asia 2025 Grand Challenge: Multimodal Multiethnic Talking-Head Video Generation</h1>


                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- Data Link. -->
                                <span class="link-block">
                                    <a href="#introduction"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span>Introduction</span>
                                    </a>
                                </span>
                                <!-- Data Link. -->
                                <span class="link-block">
                                    <a href="#dataset"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span>Dataset</span>
                                    </a>
                                </span>
                                <!-- Paper Link. -->
                                <span class="link-block">
                                    <a href="#task"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span>Task & Evaluation</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="#submission"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span>Participate</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <a href="#timeline"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span>Timeline</span>
                                    </a>
                                </span>

                                <!-- <span class="link-block">
                                    <a href="https://www.codabench.org/competitions/4897/"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span>CodaBench</span>
                                    </a>
                                </span> -->

                                <span class="link-block">
                                    <a href="https://github.com/luna-ai-lab/DH-FaceVid-1K"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span>Project</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    
  <!-- æ ‡é¢˜å·²ä¿®æ”¹ä¸ºæ¬¢è¿è¯­ -->
<!-- START: éœ“è™¹å¹»å½©æ¬¢è¿è¯­ (è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€è‡ªåŒ…å«çš„ä»£ç å—) -->
<div style="text-align: center;">
    <style>
        /* åŠ¨ç”»æ•ˆæœå¿…é¡»åœ¨è¿™é‡Œå®šä¹‰ */
        @keyframes animate-gradient-welcome {
            0% { background-position: 0% 50%; }
            50% { background-position: 100% 50%; }
            100% { background-position: 0% 50%; }
        }
    </style>
    <p style="
        font-size: 2em; 
        margin: 2em 0; 
        font-weight: bold;
        /* --- æ ¸å¿ƒå¹»å½©æ•ˆæœæ ·å¼ --- */
        background: linear-gradient(90deg, #ff00de, #00f2ff, #ff8f00, #ff00de);
        background-size: 400% 400%;
        -webkit-background-clip: text;
        background-clip: text;
        -webkit-text-fill-color: transparent;
        animation: animate-gradient-welcome 10s ease-in-out infinite;
        text-shadow: 
            0 0 5px rgba(255, 255, 255, 0.5),
            0 0 10px #ff00de,
            0 0 20px #00f2ff;
    ">
        Registration is now open. We warmly welcome your registration and participation!
    </p>
</div>
<!-- END: éœ“è™¹å¹»å½©æ¬¢è¿è¯­ -->
<!-- 
    ã€é‡è¦ã€‘ä¸‹é¢çš„åŸå§‹å›¢é˜Ÿåˆ—è¡¨å·²è¢«å®Œå…¨æ³¨é‡Šæ‰ã€‚
    å½“æœ‰å›¢é˜Ÿæ³¨å†Œåï¼Œæ‚¨åªéœ€åˆ é™¤å¼€å¤´ <!-- å’Œç»“å°¾ --> 

<!--
<div class="team-container">
    <div class="team-card">Heâ€˜s mates</div>
    <div class="team-card">STU-ZZG</div>
    <div class="team-card">DZ</div>
    <div class="team-card">smiles lab acmmm2025</div>
    <div class="team-card">EchoEmotion</div>
    <div class="team-card">xAGI</div>
    <div class="team-card">AI4AI</div>
    <div class="team-card">HaHaHa Team</div>
    <div class="team-card">CCNU-team</div>
    <div class="team-card">SYSU_RUNNER</div>
    <div class="team-card">Timi</div>
    <div class="team-card">IMS</div>
    <div class="team-card">GZYZWYQH</div>
    <div class="team-card">NeuChamp</div>
    <div class="team-card">TalkingHead-Runner</div>
    <div class="team-card">free style</div>
    <div class="team-card">ERC</div>
    <div class="team-card">SuperEmoNLP</div>
    <div class="team-card">PKBJ</div>
    <div class="team-card">It's MyGO!!!!!</div>
</div>
-->
<div class="notification is-warning is-light has-text-centered">
    <strong>ğŸ“¢ Important:</strong> All participants are requested to review the 
<a href="https://docs.google.com/document/d/1Wo-g8iyrT8gNPCm3Tro-J5eZlWqQRTVuSN2nowwI0iQ/edit?tab=t.0#heading=h.5405osv0wrj1">Submission Guidelines</a>.
    </div>
    <!-- News Section -->
    <section class="news">
        <div class="container is-max-desktop">
          <div id="news" class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">News & Updates</h2>
              <div class="content has-text-left">
                <ul style="list-style-position: outside; padding-left: 1.5em;">
  
                    <!-- ç¬¬ä¸€æ¡ï¼šæœ‰æ˜¾ç¤ºçš„å›¾æ ‡ -->
                    <li style="text-indent: -1.5em; padding-left: 1.5em; margin-bottom: 0.5em;">
                      <span style="display: inline-block; width: 1.5em;">ğŸ†•</span><strong>December 3, 2025:</strong> The detailed program for the challenge session has been officially released.
                    </li>
                  
                    <!-- ç¬¬äºŒæ¡ï¼šä½¿ç”¨ visibility: hidden åˆ¶ä½œéšå½¢å ä½ç¬¦ -->
                    <li style="text-indent: -1.5em; padding-left: 1.5em;">
                      <span style="display: inline-block; width: 1.5em; visibility: hidden;">ğŸ†•</span><strong>October 15, 2025:</strong> The final ranking and results of the challenge have been released. Congratulations to the winning teams!
                    </li>
                  
                  </ul>
              </div>
            </div>
          </div>
        </div>
      </section>

    <section class="section">
        <div class="container is-max-desktop">

            <!-- <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Leaderboard</h2>
                    <div class="content has-text-justified">
                        <p>
                            Congratulations to the winners!
                        </p>
                        <table class="._table table-bordered table-striped" align="center">
                            <tbody align="center" valign="center">
                            <tr>
                                <td>Rank</td>
                                <td>Team</td>
                                <td>Score</td>
                              </tr>
                            <tr>
                              <td>1</td>
                              <td>Token</td>
                              <td>63.2064</td>
                            </tr>
                            <tr>
                              <td>2</td>
                              <td>USTC-IAT-United</td>
                              <td>62.1149</td>
                            </tr>
                            <tr>
                                <td>3</td>
                                <td>ppjj</td>
                                <td>59.8638</td>
                            </tr>
                            <tr>
                                <td>4</td>
                                <td>GXU-LIPE</td>
                                <td>59.6864</td>
                            </tr>
                            <tr>
                                <td>5</td>
                                <td>DMCV</td>
                                <td>59.3998</td>
                            </tr>
                            
                        </tbody>
                        </table>
                    </div>
                </div>
            </div> -->
            
            <!-- Abstract. -->
            <div id="introduction" class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Introduction</h2>
                    <div class="content has-text-justified">

                        <div style="text-align: center;">
                            <img src="static\files\figure1.PNG" width="90%" alt="">
                            <p><b>Overview: Talking-Head Generation.</b></p>
                        </div>

                        <p>
                            We propose the "Multimodal Multiethnic Talking-Head Video Generation" Grand Challenge on the ACM Multimedia Asia 2025 platform. This challenge aims to advance the state-of-the-art beyond models that often exhibit poor generalization across different ethnicities and suffer from slow inference speeds. Given a static reference face and multimodal prompts (such as audio or text), models are expected to generate high-quality, realistic, and expressive talking-head videos that feature strong identity consistency, accurate lip synchronization, and controllable speaking styles.
                        </p>

                    </div>
                </div>
            </div>


            <!-- Paper video. -->
            <div id="task" class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Challenge Task Definition</h2>
                    <div class="content has-text-justified">
            
                        <p><b>Task: High-Quality Talking-Head Video Generation</b></p>
                        
                        <!-- æ–°å¢çš„å›¾ç‰‡ -->
                        <div style="text-align: center;">
                            <img src="static/files/task.jpg" width="90%" alt="Challenge Task Overview">
                            <br> <!-- æ·»åŠ ä¸€ä¸ªæ¢è¡Œä»¥å¢åŠ ä¸ä¸‹æ–¹æ–‡æœ¬çš„é—´è· -->
                        </div>
            
                        <p>
                            Participants are required to train a generalizable, high-performance talking-head video generation model. While we recommend using the provided <b>DH-FaceVid-1K dataset</b> for training, participants are free to use any additional or alternative training datasets. There are no restrictions on the technical architecture or experimental setups. This means that for each test sample, beyond the provided reference face image and speaking audio, participants are free to choose the input modalities for their model (e.g., using phoneme labels or text prompts derived from the audio). Furthermore, we do not impose any limitations on the training hardware configurations, such as the operating system or graphics cards used.
                        </p>
                     
                        <p>
                            After completing model training, participants are required to use their trained model to perform inference on the test set samples, which are provided in the official Hugging Face repository: <a href="https://huggingface.co/datasets/jjuik2014/MMAsia-Challenge-Testset" target="_blank">https://huggingface.co/datasets/jjuik2014/MMAsia-Challenge-Testset</a>. Subsequently, they must submit the generated talking-head videos as instructed in the Submission Guidelines. Optionally, participants may create an online demo page to facilitate our evaluation of each method's specific inference speed.
                        </p>
                        
                    </div>
                </div>
            </div>
            <!--/ Challenge Task Definition. -->

<!-- Evaluation Metrics. -->
<div id="participation" class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Participation</h2>
    
    <!-- åŸæœ‰çš„æ³¨å†Œè¯´æ˜ -->
    <div class="content has-text-justified">
      <p>
        The registration process is simple. Please submit your team name via this Google Form: <a href="https://docs.google.com/forms/d/1w2vO4VlJefpM7KPodLJr3omOMIkLgaDG8LahCKH4Eqw" target="_blank">https://docs.google.com/forms/d/1w2vO4VlJefpM7KPodLJr3omOMIkLgaDG8LahCKH4Eqw</a>. You can use a placeholder for the results link during the initial submission and update it later.
      </p>
    </div>

    <!-- æ–°å¢ï¼šå·²æ³¨å†Œé˜Ÿä¼è¡¨æ ¼ -->
    <div class="content">
      <h4 class="title is-5 mt-5">Registered Teams</h4>
      <div class="table-container">
        <table class="table is-bordered is-striped is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th class="has-text-centered" style="width: 20%;">ID</th>
              <th class="has-text-centered">Team Name</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1</td>
              <td>3dScanning</td>
            </tr>
            <tr>
              <td>2</td>
              <td>AnthGen</td>
            </tr>
            <tr>
              <td>3</td>
              <td>cmvs</td>
            </tr>
            <tr>
              <td>4</td>
              <td>DHAL</td>
            </tr>
            <tr>
              <td>5</td>
              <td>HairBrother_Team</td>
            </tr>
            <tr>
              <td>6</td>
              <td>SZFaceU</td>
            </tr>
            <tr>
              <td>7</td>
              <td>Talk_Sclab</td>
            </tr>
            <tr>
              <td>8</td>
              <td>Team_EQUES</td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>

  </div>
</div>
<div id="evaluation" class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2 class="title is-3">Evaluation Metrics</h2>
        <div class="content has-text-justified">

            <p><strong>Objective Metrics</strong></p>
            <p>
                Below are the objective metrics evaluated on the test set of DH-FaceVid-1K. We will use these metrics to automatically evaluate the performance of the submitted test set samples. Participants can also use them to evaluate model performance during training.
            </p>
            <ul>
                <li><strong>FID (FrÃ©chet Inception Distance)</strong></li>
                <li><strong>FVD (FrÃ©chet Video Distance)</strong></li>
                <li><strong>Inference Speed (frames per second, FPS)</strong></li>
                <li><strong>Sync-C & Sync-D (Synchronization Confidence & Distance for lip-sync quality)</strong></li>
                <li><strong>AKD (Average Keypoint Distance for facial motion accuracy)</strong></li>
                <li><strong>F-LMD (Facial Landmark Distance for expression fidelity)</strong></li>
            </ul>
            <p>
                These metrics quantitatively assess generation quality, temporal coherence, speed, and speaking style transferability in face video synthesis.
            </p>

            <br> 

            <p><strong>Subjective Evaluation</strong></p>
            <p>
                For manual subjective evaluation, we will assess the following aspects in the videos provided by the authors: <strong>identity consistency, video quality, lip synchronization, and speaking style controllability</strong>. Each item has a full score of 5 points.
            </p>

            <br>

            <p><strong>Final Ranking</strong></p>
            <p>
                The final rankings of participants will be determined based on a combination of these objective and subjective scores, along with the model's inference speed demonstrated on the optional web demo page.
            </p>

        </div>
    </div>
</div>
<!--/ Evaluation Metrics. -->
 
             <!-- Datasets. -->
             <div class="columns is-centered has-text-centered">
                <div id="dataset" class="column is-four-fifths">
                    <h2 class="title is-3">Dataset</h2>
                    <div class="content has-text-justified">
                        <p>
                            The dataset for this challenge is <b>DH-FaceVid-1K: A Large-Scale High-Quality Dataset for Face Video Generation</b>.
                        </p>
                        <p>
                            The dataset is available for download on the official GitHub repository: <a href="https://github.com/luna-ai-lab/DH-FaceVid-1K" target="_blank">https://github.com/luna-ai-lab/DH-FaceVid-1K</a>.
                        </p>
            
                        <!-- æ•°æ®é›†æ¦‚è§ˆå›¾ç‰‡ -->
                        <div style="text-align: center;">
                            <img src="static\files\sample.jpg" width="90%" alt="Overview of the DH-FaceVid-1K dataset">
                            <br>
                        </div>
            
                        <!-- å…³äºæµ‹è¯•é›†æ ·æœ¬çš„æè¿° -->
                        <p>
                            For the test set samples, we provide the reference face image, speaking audio, and corresponding text annotations. An example is shown above.
                        </p>
            
                        <!-- æµ‹è¯•é›†æ ·æœ¬ç¤ºä¾‹å›¾ç‰‡ -->
                    </div>
                </div>
            </div>

            <!DOCTYPE html>
<html>
<head>
    <title>Timeline with AoE Time</title>
    <!-- ä¸ºäº†è®©æ ·å¼ç”Ÿæ•ˆï¼Œæˆ‘åŠ å…¥äº†ä¸€ä¸ªç®€å•çš„CSSæ¡†æ¶çš„é“¾æ¥ -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <style>
        /* æ‚¨å¯ä»¥æ·»åŠ è‡ªå®šä¹‰æ ·å¼ */
        #current-time-container {
            margin-bottom: 1rem;
            font-size: 1.1em;
        }
        #current-time {
            font-weight: bold;
            color: #d9534f; /* çº¢è‰²ä»¥çªå‡ºæ˜¾ç¤º */
        }
    </style>
</head>
<body>
    <div class="columns is-centered has-text-centered">
        <div id="timeline" class="column is-four-fifths">
            <h2 class="title is-3">Timeline</h2>
            
            <!-- æ˜¾ç¤ºå½“å‰ AoE æ—¶é—´ -->
            <p id="current-time-container">
                <strong>Current Time (Anywhere on Earth, UTC-12):</strong> 
                <span id="current-time"></span>
            </p>
    
            <div class="content has-text-justified">
                <p>
                    Please note: The submission deadline is at 11:59 p.m. (<a href="https://www.timeanddate.com/time/zones/aoe" style="color:red">Anywhere on Earth</a>) of the stated deadline date.
                </p>
                <table class="table is-bordered is-striped" align="center">
                    <tbody align="center" valign="center">
                    <tr>
                      <td><s>Training data and participant instruction release</s></td>
                      <td><s>September 8, 2025</s></td>
                    </tr>
                    <tr>
                      <td><s>Registration deadline</s></td>
                      <td><s>September 30, 2025</s></td>
                    </tr>
                    <tr>
                        <td><s>Test dataset release</s></td>
                        <td><s>September 9, 2025</s></td>
                    </tr>
                    <tr>
                        <td><s>Result submission start</s></td>
                        <td><s>September 10, 2025</s></td>
                    </tr>
                    <tr>
                        <td><s>Result submission end</s></td>
                        <td><s>September 30, 2025</s></td>
                    </tr>
                    <tr>
                        <td><s>Paper invitation notification</s></td>
                        <td><s>October 1, 2025</s></td>
                    </tr>
                    <tr>
                        <td><s>Paper submission deadline</s></td>
                        <td><s>October 14, 2025</s></td>
                    </tr>
                    <tr>
                        <td><s>Camera-ready paper</s></td>
                        <td><s>October 24, 2025</s></td>
                    </tr>
                    
                    <!-- æ–°å¢ï¼šä¼šè®®æ—¶é—´ -->
                    <tr>
                        <td><strong>ACM Multimedia Asia 2025 Challenge Session</strong></td>
                        <td><strong>December 9, 2025</strong></td>
                    </tr>
    
                </tbody>
                </table>
            </div>
        </div>
    </div>

    <!-- JavaScript ä»£ç ï¼Œç”¨äºå®æ—¶æ›´æ–° AoE æ—¶é—´ -->
    <script>
        function updateAoeTime() {
            // 1. è·å–å½“å‰æ—¶é—´çš„ Date å¯¹è±¡
            const now = new Date();
            
            // 2. è®¡ç®— AoE æ—¶é—´ (UTC-12)
            // é¦–å…ˆè·å–å½“å‰æ—¶é—´çš„UTCæ—¶é—´æˆ³ï¼Œç„¶åå‡å»12å°æ—¶çš„æ¯«ç§’æ•°
            const aoeTime = new Date(now.getTime() - (12 * 60 * 60 * 1000));
            
            // 3. æ ¼å¼åŒ– AoE æ—¶é—´ä¸ºå­—ç¬¦ä¸² (YYYY-MM-DD HH:MM:SS)
            // ä½¿ç”¨ toISOString() æ–¹ä¾¿åœ°è·å– UTC æ—¶é—´æ ¼å¼ï¼Œç„¶åè¿›è¡Œå¤„ç†
            const isoString = aoeTime.toISOString(); // æ ¼å¼å¦‚: "2025-09-08T14:29:00.000Z"
            const formattedAoeTime = isoString.substring(0, 10) + ' ' + isoString.substring(11, 19);

            // 4. å°†æ ¼å¼åŒ–åçš„æ—¶é—´æ›´æ–°åˆ° HTML å…ƒç´ ä¸­
            document.getElementById('current-time').textContent = formattedAoeTime;
        }
        
        // é¡µé¢åŠ è½½åç«‹å³æ‰§è¡Œä¸€æ¬¡ï¼Œé¿å…å»¶è¿Ÿæ˜¾ç¤º
        updateAoeTime();
        
        // æ¯ç§’é’Ÿæ›´æ–°ä¸€æ¬¡æ—¶é—´
        setInterval(updateAoeTime, 1000);
    </script>
</body>
</html>



<!-- 
[
{
    "id": "test-0",
    "event": [{
        "trigger": "advance",
        "type": "Transport",
        "arguments": [{
            "name": "elements",
            "role": "Artifact"
        }, {
            "name": "city",
            "role": "Origin"
        }, {
            "name": "Baghdad",
            "role": "Destination"
        }]
    }]
},
{
    "id": "test-1",
    "event": [{
       ...
    }]
},
...
]
 -->
</pre>
                      
<!--                         <p>
                            Link to <a href="#">Codalab</a>
                        </p>
                        <p>
                            
                            Participants can submit at <a href="https://www.codabench.org/competitions/">Codabench</a>.
                        </p>
                        <p>
                            
                            We will review the submissions publish the ranking here. 
                            Feel free to contact us at <a href="22021110280@stu.xidian.edu.cn">email</a>.
                            
                        
                        </p> -->
<!-- 
                        <p> Comming Soon </p>
                    </div>
                </div>
            </div> -->

            <!-- / Paper video. -->
            <div class="columns is-centered has-text-centered">
                <div id="baseline" class="column is-four-fifths">
                    <h2 class="title is-3">Baseline</h2>
                    <div class="content has-text-justified">
                      
                        <p>
                            Link to the provided training dataset repository <a href="https://github.com/luna-ai-lab/DH-FaceVid-1K">DH-FaceVid-1K</a>.
                        </p>
                    </div>
                </div>
            </div>
            
        
             <!-- <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Registration</h2>
                    <div class="content has-text-justified">
                        <p>
                             Welcome and please apply for the VSD challenge via a form at <a href="https://docs.google.com/forms/d/e/1FAIpQLSfuKXhaN5qa61rWlwaE3VpJD5FHcVLH25Un95wch6fCKiXIGQ/viewform?usp=sf_link">this link</a>.
                        </p>
                        <p>
                            Feel free to contact us at <a href="mailto: vsdchallenge@gmail.com">vsdchallenge@gmail.com</a>.
                        </p>
                    </div>
                </div>
            </div> -->

            
            
            
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Challenge Results</h2>
                    <div class="content has-text-centered">
                        <ul style="list-style: none; padding: 0;">
                            <li style="margin-bottom: 10px;">
                                <span style="font-size: 1.3em; color: #d4af37;">ğŸ† <strong>1st Place:</strong> CMVS</span>
                            </li>
                            <li style="margin-bottom: 10px;">
                                <span style="font-size: 1.2em; color: #a9a9a9;">ğŸ¥ˆ <strong>2nd Place:</strong> SZFaceU</span>
                            </li>
                            <li style="margin-bottom: 10px;">
                                <span style="font-size: 1.2em; color: #cd7f32;">ğŸ¥‰ <strong>3rd Place:</strong> EQUES</span>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="column is-four-fifths">
                <h2 class="title is-3 has-text-centered">Challenge Session Program</h2>
                <p class="has-text-centered"><strong>Date:</strong> December 9, 2025</p>
            
                <div class="content">
                    <h4 class="title is-5">09:30 - 10:00: Opening & Awards</h4>
                    <ul>
                        <li><strong>09:30 - 09:40:</strong> Opening Remarks & Challenge Introduction</li>
                        <li><strong>09:40 - 10:00:</strong> Award Presentation</li>
                    </ul>
            
                    <h4 class="title is-5">10:30 - 11:30: Team Presentations</h4>
            
                    <!-- Team 3 (EQUES) -->
                    <div class="box">
                        <p><strong>10:30 - 10:50 | 3rd Place: Team EQUES</strong></p>
                        <p><strong>Title:</strong> AKITalk: Audio-Implicit Keypoints for Identity-Preserving Talking-Head Video Synthesis</p>
                        <p><strong>Presenter:</strong> Riku Takahashi (HOSEI University)</p>
                        <details>
                            <summary>Click to view Abstract</summary>
                            <p style="font-size: 0.9em; margin-top: 10px;">
                                Existing talking-head video generation methods using speech audio often suffer from high computational costs or degraded identity preservation due to reliance on external super-resolution models. To address these issues, we propose a lightweight framework that predicts temporally consistent implicit 3D features from a reference image and speech audio. These features guide a generator trained on large-scale data to synthesize high-quality video frames efficiently. Experimental results demonstrate that our method achieves comparable or superior visual quality and identity preservation, while ensuring high inference efficiency. This balance of realism and computational performance suggests broad applicability in real-world scenarios such as virtual avatars and social media.
                            </p>
                        </details>
                    </div>
            
                    <!-- Team 2 (SZFaceU) -->
                    <div class="box">
                        <p><strong>10:50 - 11:10 | 2nd Place: Team SZFaceU</strong></p>
                        <p><strong>Title:</strong> FlowTalk: Real-Time Audio-Driven Talking Head Synthesis via Motion-Space Flow Matching</p>
                        <p><strong>Presenter:</strong> Kaijun Deng (Shenzhen University)</p>
                        <details>
                            <summary>Click to view Abstract</summary>
                            <p style="font-size: 0.9em; margin-top: 10px;">
                                Audio-driven talking head synthesis has achieved significant progress, yet existing methods face critical trade-offs among generation quality, inference efficiency, and cross-ethnic generalization. Diffusion-based approaches produce high-fidelity results but suffer from slow inference due to iterative denoising, while GAN-based methods achieve faster speed at the cost of reduced motion naturalness and limited generalization. To address these challenges, we propose FlowTalk, a novel framework that enables real-time high-fidelity talking head video synthesis. Our approach leverages Flow Matching technology to perform efficient motion modeling in a decoupled motion space rather than pixel space, achieving significant speedup while maintaining generation quality. Specifically, we adopt an off-the-shelf motion extractor to disentangle facial appearance from motion, and employ an OT-based flow matching model with a transformer architecture to predict identity-agnostic motion sequences conditioned on audio features. To improve cross-ethnic generalization, we train on a balanced combination of DH-FaceVid-1K and HDTF datasets with HuBert-CN as the audio encoder. Experimental results demonstrate that FlowTalk achieves over 100 FPS with 32 ODE solver steps, approximately 5 times faster than diffusion-based baselines with 500 steps, while preserving comparable visual quality in lip synchronization, facial expressions, and head movements.
                            </p>
                        </details>
                    </div>
            
                    <!-- Team 1 (CMVS) -->
                    <div class="box">
                        <p><strong>11:10 - 11:30 | 1st Place: Team CMVS</strong></p>
                        <p><strong>Title:</strong> Detection-Aware Inference for Robust Talking-Head Video Generation</p>
                        <p><strong>Presenter:</strong> Yueyi Yang (In-person)</p>
                        <details>
                            <summary>Click to view Abstract</summary>
                            <p style="font-size: 0.9em; margin-top: 10px;">
                                Talking-head video generation has seen significant progress with the rise of multimodal and multiethnic datasets; however, existing systems still suffer from stability issues when dealing with occluded or partially missing facial regions in the input. In this work, we present our model developed for the ACM Multimedia Asia 2025 Grand Challenge on Multimodal Multiethnic Talking Head Video Generation. Built upon the Hallo2 framework, our model introduces two key improvements to enhance inference robustness and usability. First, it detects cases where facial regions are heavily occluded or incompletely detected and automatically substitutes a static image across all frames, preventing inference failures and maintaining visual consistency when facial information is insufficient. Second, it supports multi-input inference, enabling the generation of multiple talking-head videos within a single execution process. These modifications result in a more reliable and flexible talking-head generation pipeline, suitable for diverse multimodal datasets and large-scale evaluation.
                            </p>
                        </details>
                    </div>
            
                </div>
            </div>
            
            <!-- / Paper video. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Organizers</h2>
                    <div class="content has-text-justified">
                        
                        <p><a href="mailto:didonglin@lixiang.com">Donglin Di</a> (Li Auto)</p>
                        <p><a href="mailto:lei.fan1@unsw.edu.au">Lei Fan</a> (The University of New South Wales)</p>
                        <p><a href="mailto:thsu@hit.edu.cn">Tonghua Su</a> (Harbin Institute of Technology)</p>
                        <p><a href="mailto:xyang21@ustc.edu.cn">Xun Yang</a> (University of Science and Technology of China)</p>
                        <p><a href="mailto:maguire9993@gmail.com">Yongjia Ma</a> (Li Auto)</p>
                        <p><a href="mailto:24b903117@stu.hit.edu.cn">He Feng</a> (Harbin Institute of Technology)</p>
            
                    </div>
                </div>
            </div>


            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">References</h2>
                    <div class="content has-text-justified">
                        <p>                     
                            [1] Di D, Feng H, Sun W, et al. DH-FaceVid-1K: A Large-Scale High-Quality Dataset for Face Video Generation[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025.
                        </p>
                     
                    </div>
                </div>
            </div>
        </div>
    </section>



</body>
<script>
    // function jsonShowFn(json){
    //     if (!json.match("^\{(.+:.+,*){1,}\}$")) {
    //         return json           //åˆ¤æ–­æ˜¯å¦æ˜¯jsonæ•°æ®ï¼Œä¸æ˜¯ç›´æ¥è¿”å›
    //     }

    //     if (typeof json != 'string') {
    //         json = JSON.stringify(json, undefined, 2);
    //     }
    //     json = json.replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/>/g, '&gt;');
    //     return json.replace(/("(\\u[a-zA-Z0-9]{4}|\\[^u]|[^\\"])*"(\s*:)?|\b(true|false|null)\b|-?\d+(?:\.\d*)?(?:[eE][+\-]?\d+)?)/g, function(match) {
    //         var cls = 'number';
    //         if (/^"/.test(match)) {
    //             if (/:$/.test(match)) {
    //                 cls = 'key';
    //             } else {
    //                 cls = 'string';
    //             }
    //         } else if (/true|false/.test(match)) {
    //             cls = 'boolean';
    //         } else if (/null/.test(match)) {
    //             cls = 'null';
    //         }
    //         return '<span class="' + cls + '">' + match + '</span>';
    //     });
    // }
    // $('#jsonShow').html(jsonShowFn('[{"imd_id":1,"triple_list":["s":"book","o":"table"]}]'));
</script>
</html>
